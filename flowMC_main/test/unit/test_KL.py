from sklearn.neighbors import KernelDensity
from sklearn.mixture import GaussianMixture
from scipy.special import kl_div
import numpy as np
import jax
import jax.random as jr
import jax.numpy as jnp  # JAX NumPy


def compute_model_kl(true_data, model_samples, model):
    """
    Calculate the KL divergence between the true data distribution and the model distribution

    Parameters:
        true_data: Real data samples (jax.Array)
        model_samples: Samples generated by the model (jax.Array)
        model: Trained flow model

    Returns:
        kl_divergence: Estimated value of KL divergence
    """
    # Convert to NumPy arrays
    true_data_np = np.array(true_data)
    model_samples_np = np.array(model_samples)

    # Use KDE to estimate the true distribution
    kde_true = KernelDensity(kernel='gaussian', bandwidth='scott')
    kde_true.fit(true_data_np)

    # Use KDE to estimate the model distribution
    kde_model = KernelDensity(kernel='gaussian', bandwidth='scott')
    kde_model.fit(model_samples_np)

    # Prepare evaluation points - combine real data and model samples
    eval_points = np.vstack([true_data_np, model_samples_np])

    # Calculate log probability density of the true distribution (KDE estimation)
    log_p_true = kde_true.score_samples(eval_points)
    p_true = np.exp(log_p_true)

    # Calculate log probability density of the model distribution (KDE estimation)
    log_q_model = kde_model.score_samples(eval_points)
    q_model = np.exp(log_q_model)

    # Avoid zero probability issue
    epsilon = 1e-10
    p_true = np.clip(p_true, epsilon, None)
    q_model = np.clip(q_model, epsilon, None)

    # Calculate KL divergence
    kl_values = kl_div(p_true, q_model)
    kl_divergence = np.mean(kl_values)

    return kl_divergence


# def calculate_kl(data, samples, model, n_dim):
#     """Calculate KL divergence for high-dimensional continuous distributions"""
#     # Convert data to numpy array
#     data_np = np.array(data)
#     samples_np = np.array(samples)
#
#     # Create kernel density estimation (suitable for low-dimensional data)
#     if n_dim <= 4:
#         kde = gaussian_kde(data_np.T)
#         log_p = kde.logpdf(samples_np.T)
#     else:  # Use flow model to estimate real distribution for high-dimensional data
#         raise NotImplementedError("Normalized flows are recommended for estimating real distributions in high dimensions")
#
#     # Get probabilities of generated samples
#     log_q = model.log_prob(samples)
#
#     # Calculate KL divergence (D_KL(P||Q) = E_P[log(p/q)] â‰ˆ mean(log_p - log_q))
#     kl_divergence = np.mean(log_p - log_q)
#     return kl_divergence


def test_kl_computation():
    """Verify the correctness of the KL divergence calculation function"""
    print("=== Verifying KL Calculation Function ===")

    # Test 1: Same distribution - KL should be close to 0
    print("\nTest 1: Same distribution (should be close to 0)")
    key = jr.key(42)
    data = jr.normal(key, (10000, 4))  # Standard normal distribution

    # Create a "pseudo-model"
    class MockModel:
        def log_prob(self, x):
            return jax.scipy.stats.norm.logpdf(x).sum(axis=1)

    model = MockModel()

    # Generate model samples (same as real data)
    kl_same = compute_model_kl(data, data, model)
    print(f"KL divergence for same distributions: {kl_same:.6f} (should be close to 0)")

    # Test 2: Different distributions - there should be significant differences
    print("\nTest 2: Different distributions (should show significant difference)")
    # Create an obviously different distribution (both mean and variance differ)
    shifted_data = data * 1.5 + 2.0
    kl_diff = compute_model_kl(data, shifted_data, model)
    print(f"KL divergence for different distributions: {kl_diff:.6f} (should be significantly greater than 0)")

    # Test 3: Distributions with known KL values
    print("\nTest 3: Distributions with known KL values")
    # Create two Gaussian Mixture Models (GMMs)
    def create_gmm(means, covs, weights, n_samples):
        gmm = GaussianMixture(
            n_components=len(means),
            covariance_type='full',
            means_init=means,
            weights_init=weights
        )
        # Set covariance matrices
        gmm.covariances_ = covs
        gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(covs))
        samples, _ = gmm.sample(n_samples)
        return jnp.array(samples)

    # Define two different GMMs
    means1 = np.array([[0, 0, 0, 0], [2, 2, 2, 2]])
    covs1 = np.array([np.eye(4), np.eye(4)])
    weights1 = np.array([0.7, 0.3])

    means2 = np.array([[0, 0, 0, 0], [3, 3, 3, 3]])
    covs2 = np.array([np.eye(4) * 0.5, np.eye(4) * 1.5])
    weights2 = np.array([0.5, 0.5])

    data1 = create_gmm(means1, covs1, weights1, 10000)
    data2 = create_gmm(means2, covs2, weights2, 10000)

    # Calculate KL divergence
    kl_gmm = compute_model_kl(data1, data2, model)
    print(f"KL divergence between different GMMs: {kl_gmm:.6f} (should be greater than 0.5)")

    # Test 4: Numerical integration verification for one-dimensional case
    print("\nTest 4: Numerical Integration Verification for One-Dimensional Case")
    # Define two one-dimensional Gaussian distributions
    mean1, std1 = 0.0, 1.0
    mean2, std2 = 1.0, 1.0

    # Theoretical KL value: KL(N(0,1)||N(1,1)) = 0.5
    x = np.linspace(-5, 5, 10000).reshape(-1, 1)
    p = np.exp(jax.scipy.stats.norm.logpdf(x, mean1, std1))
    q = np.exp(jax.scipy.stats.norm.logpdf(x, mean2, std2))

    # Numerically integrate to calculate KL divergence
    kl_integral = np.trapz(p * np.log(p / q), x.flatten())

    # Calculate using our function
    samples1 = jr.normal(key, (10000, 1)) * std1 + mean1
    samples2 = jr.normal(key, (10000, 1)) * std2 + mean2
    kl_our = compute_model_kl(samples1, samples2, model)

    print(f"KL value via numerical integration: {kl_integral:.6f}")
    print(f"Calculated value using our function: {kl_our:.6f}")
    print(f"Theoretical value: 0.500000")
    print(f"Difference: {abs(kl_our - 0.5):.6f}")


# Run validation tests
test_kl_computation()
